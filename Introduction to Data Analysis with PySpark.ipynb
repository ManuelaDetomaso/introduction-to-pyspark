{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: introduction to data analysis with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_HOME = \"C:\\Spark\\spark-3.0.1-bin-hadoop2.7\"\n",
    "findspark.init(SPARK_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration & Intialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SparkContext — provides connection to Spark with the ability to create RDDs\n",
    "* SQLContext — provides connection to Spark with the ability to run SQL queries on data\n",
    "* SparkSession — all-encompassing context which includes coverage for SparkContext, SQLContext and HiveContext. SparkSession is the entry point for programming Spark applications. It let you interact with DataSet and DataFrame APIs provided by Spark. We set the application name by calling appName. The getOrCreate() method either returns a new SparkSession of the app or returns the existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "spark = SparkSession.builder.appName(\"data analysis with PySpark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SparkContext instance which allows the Spark Application to access \n",
    "# Spark Cluster with the help of a resource manager which is usually YARN or Mesos\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SQLContext instance to access the SQL query engine built on top of Spark\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by loading the files in our dataset using the spark.read.load command. This command reads parquet files, which is the default file format for spark, but you can add the parameter format to read other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------------+-----+--------------------+---------+---------+----------+\n",
      "| case_id|province|        city|group|      infection_case|confirmed| latitude| longitude|\n",
      "+--------+--------+------------+-----+--------------------+---------+---------+----------+\n",
      "| 1000001|   Seoul|  Yongsan-gu| true|       Itaewon Clubs|      139|37.538621|126.992652|\n",
      "| 1000002|   Seoul|   Gwanak-gu| true|             Richway|      119| 37.48208|126.901384|\n",
      "| 1000003|   Seoul|     Guro-gu| true| Guro-gu Call Center|       95|37.508163|126.884387|\n",
      "| 1000004|   Seoul|Yangcheon-gu| true|Yangcheon Table T...|       43|37.546061|126.874209|\n",
      "| 1000005|   Seoul|   Dobong-gu| true|     Day Care Center|       43|37.679422|127.044374|\n",
      "| 1000006|   Seoul|     Guro-gu| true|Manmin Central Ch...|       41|37.481059|126.894343|\n",
      "+--------+--------+------------+-----+--------------------+---------+---------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import covid cases dataset\n",
    "cases = spark.read.load(\n",
    "    \"data/Case.csv\", \n",
    "    format=\"csv\", \n",
    "    sep=\",\",\n",
    "    inferSchema=\"true\", \n",
    "    header=\"true\"\n",
    ")\n",
    "cases.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+---------+--------+--------+\n",
      "|      date|time|province|confirmed|released|deceased|\n",
      "+----------+----+--------+---------+--------+--------+\n",
      "|2020-01-20|  16|   Seoul|        0|       0|       0|\n",
      "|2020-01-20|  16|   Busan|        0|       0|       0|\n",
      "|2020-01-20|  16|   Daegu|        0|       0|       0|\n",
      "|2020-01-20|  16| Incheon|        1|       0|       0|\n",
      "|2020-01-20|  16| Gwangju|        0|       0|       0|\n",
      "|2020-01-20|  16| Daejeon|        0|       0|       0|\n",
      "+----------+----+--------+---------+--------+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import cases timing dataset\n",
    "time_province = spark.read.load(\n",
    "    \"data/TimeProvince.csv\", \n",
    "    format=\"csv\", \n",
    "    sep=\",\",\n",
    "    inferSchema=\"true\", \n",
    "    header=\"true\"\n",
    ")\n",
    "time_province.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------+---------+----------+-----------------------+------------------+----------------+-------------+------------------------+-------------------+------------------+\n",
      "| code|province|       city| latitude| longitude|elementary_school_count|kindergarten_count|university_count|academy_ratio|elderly_population_ratio|elderly_alone_ratio|nursing_home_count|\n",
      "+-----+--------+-----------+---------+----------+-----------------------+------------------+----------------+-------------+------------------------+-------------------+------------------+\n",
      "|10000|   Seoul|      Seoul|37.566953|126.977977|                    607|               830|              48|         1.44|                   15.38|                5.8|             22739|\n",
      "|10010|   Seoul| Gangnam-gu|37.518421|127.047222|                     33|                38|               0|         4.18|                   13.17|                4.3|              3088|\n",
      "|10020|   Seoul|Gangdong-gu|37.530492|127.123837|                     27|                32|               0|         1.54|                   14.55|                5.4|              1023|\n",
      "|10030|   Seoul| Gangbuk-gu|37.639938|127.025508|                     14|                21|               0|         0.67|                   19.49|                8.5|               628|\n",
      "|10040|   Seoul| Gangseo-gu|37.551166|126.849506|                     36|                56|               1|         1.17|                   14.39|                5.7|              1080|\n",
      "|10050|   Seoul|  Gwanak-gu| 37.47829|126.951502|                     22|                33|               1|         0.89|                   15.12|                4.9|               909|\n",
      "+-----+--------+-----------+---------+----------+-----------------------+------------------+----------------+-------------+------------------------+-------------------+------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import regions dataset\n",
    "regions = spark.read.load(\n",
    "    \"data/Region.csv\",\n",
    "    format=\"csv\", \n",
    "    sep=\",\", \n",
    "    inferSchema=\"true\", \n",
    "    header=\"true\"\n",
    ")\n",
    "regions.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative reading options\n",
    "#df = spark.read.csv(\"Case.csv\", header=True, inferSchema=True)\n",
    "#df = spark.read.format('csv').options(header=True,inferSchema=True).load(\"Case.csv\")\n",
    "\n",
    "# for other file formats:\n",
    "#df = spark.read.text(path_to_file)\n",
    "#df = spark.read.json(path_to_file)\n",
    "#df = spark.read.parquet(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+-----+--------------------------------+---------+---------+----------+\n",
      "| case_id|province|city           |group|infection_case                  |confirmed|latitude |longitude |\n",
      "+--------+--------+---------------+-----+--------------------------------+---------+---------+----------+\n",
      "|1000001 |Seoul   |Yongsan-gu     |true |Itaewon Clubs                   |139      |37.538621|126.992652|\n",
      "|1000002 |Seoul   |Gwanak-gu      |true |Richway                         |119      |37.48208 |126.901384|\n",
      "|1000003 |Seoul   |Guro-gu        |true |Guro-gu Call Center             |95       |37.508163|126.884387|\n",
      "|1000004 |Seoul   |Yangcheon-gu   |true |Yangcheon Table Tennis Club     |43       |37.546061|126.874209|\n",
      "|1000005 |Seoul   |Dobong-gu      |true |Day Care Center                 |43       |37.679422|127.044374|\n",
      "|1000006 |Seoul   |Guro-gu        |true |Manmin Central Church           |41       |37.481059|126.894343|\n",
      "|1000007 |Seoul   |from other city|true |SMR Newly Planted Churches Group|36       |-        |-         |\n",
      "|1000008 |Seoul   |Dongdaemun-gu  |true |Dongan Church                   |17       |37.592888|127.056766|\n",
      "|1000009 |Seoul   |from other city|true |Coupang Logistics Center        |25       |-        |-         |\n",
      "|1000010 |Seoul   |Gwanak-gu      |true |Wangsung Church                 |30       |37.481735|126.930121|\n",
      "+--------+--------+---------------+-----+--------------------------------+---------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show dataset with full columns names\n",
    "cases.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' case_id',\n",
       " 'province',\n",
       " 'city',\n",
       " 'group',\n",
       " 'infection_case',\n",
       " 'confirmed',\n",
       " 'latitude',\n",
       " 'longitude']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |--  case_id: integer (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- group: boolean (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- confirmed: integer (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# know data schema\n",
    "cases.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a descriptive overview of data fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------------+--------------------+------------------+------------------+------------------+\n",
      "|summary|province|           city|      infection_case|         confirmed|          latitude|         longitude|\n",
      "+-------+--------+---------------+--------------------+------------------+------------------+------------------+\n",
      "|  count|     174|            174|                 174|               174|               174|               174|\n",
      "|   mean|    null|           null|                null| 65.48850574712644| 36.69405111076924|127.58488500461536|\n",
      "| stddev|    null|           null|                null|355.09765388939746|0.9114662922487264| 0.823086807800544|\n",
      "|    min|   Busan|              -|Anyang Gunpo Past...|                 0|                 -|                 -|\n",
      "|    max|   Ulsan|from other city|     overseas inflow|              4511|         37.758635|          129.1256|\n",
      "+-------+--------+---------------+--------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe data\n",
    "describe_cases = cases.select(\"province\", \"city\", \"infection_case\", \"confirmed\", \"latitude\", \"longitude\")\n",
    "describe_cases.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter a data frame using multiple conditions using AND(&), OR(|) and NOT(~) conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter columns\n",
    "seoul_cases = cases.filter(F.col(\"province\")==\"Seoul\")\n",
    "seoul_cases.count()\n",
    "\n",
    "#alternative\n",
    "#seoul_cases = cases.where(F.col(\"province\")==\"Seoul\")\n",
    "#seoul_cases.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------------+-----+--------------------+---------+---------+----------+\n",
      "| case_id|province|        city|group|      infection_case|confirmed| latitude| longitude|\n",
      "+--------+--------+------------+-----+--------------------+---------+---------+----------+\n",
      "| 1200001|   Daegu|      Nam-gu| true|  Shincheonji Church|     4511| 35.84008|  128.5667|\n",
      "| 1200002|   Daegu|Dalseong-gun| true|Second Mi-Ju Hosp...|      196|35.857375|128.466651|\n",
      "| 1200003|   Daegu|      Seo-gu| true|Hansarang Convale...|      124|35.885592|128.556649|\n",
      "| 1200004|   Daegu|Dalseong-gun| true|Daesil Convalesce...|      101|35.857393|128.466653|\n",
      "| 1200009|   Daegu|           -|false|contact with patient|      917|        -|         -|\n",
      "| 1200010|   Daegu|           -|false|                 etc|      747|        -|         -|\n",
      "+--------+--------+------------+-----+--------------------+---------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cases.where((cases.confirmed>100) & (cases.province=='Daegu')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select a subset of columns using the select keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|province|        city|\n",
      "+--------+------------+\n",
      "|   Seoul|  Yongsan-gu|\n",
      "|   Seoul|   Gwanak-gu|\n",
      "|   Seoul|     Guro-gu|\n",
      "|   Seoul|Yangcheon-gu|\n",
      "|   Seoul|   Dobong-gu|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select columns \n",
    "cases.select([\"province\", \"city\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change a single column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+-----+--------------------+---------+---------+----------+\n",
      "| case_id|province|           city|group|    infection_source|confirmed|      lat|      long|\n",
      "+--------+--------+---------------+-----+--------------------+---------+---------+----------+\n",
      "| 1000001|   Seoul|     Yongsan-gu| true|       Itaewon Clubs|      139|37.538621|126.992652|\n",
      "| 1000002|   Seoul|      Gwanak-gu| true|             Richway|      119| 37.48208|126.901384|\n",
      "| 1000003|   Seoul|        Guro-gu| true| Guro-gu Call Center|       95|37.508163|126.884387|\n",
      "| 1000004|   Seoul|   Yangcheon-gu| true|Yangcheon Table T...|       43|37.546061|126.874209|\n",
      "| 1000005|   Seoul|      Dobong-gu| true|     Day Care Center|       43|37.679422|127.044374|\n",
      "| 1000006|   Seoul|        Guro-gu| true|Manmin Central Ch...|       41|37.481059|126.894343|\n",
      "| 1000007|   Seoul|from other city| true|SMR Newly Planted...|       36|        -|         -|\n",
      "| 1000008|   Seoul|  Dongdaemun-gu| true|       Dongan Church|       17|37.592888|127.056766|\n",
      "| 1000009|   Seoul|from other city| true|Coupang Logistics...|       25|        -|         -|\n",
      "| 1000010|   Seoul|      Gwanak-gu| true|     Wangsung Church|       30|37.481735|126.930121|\n",
      "+--------+--------+---------------+-----+--------------------+---------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename columns: exisitng left, new right\n",
    "cases = cases\\\n",
    "    .withColumnRenamed(\"latitude\", \"lat\")\\\n",
    "    .withColumnRenamed(\"longitude\", \"long\")\\\n",
    "    .withColumnRenamed(\"infection_case\", \"infection_source\")\n",
    "\n",
    "cases.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort data by increasing (deafult) or decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------------+-----+--------------------+---------+---------+----------+\n",
      "| case_id|         province|           city|group|    infection_source|confirmed|      lat|      long|\n",
      "+--------+-----------------+---------------+-----+--------------------+---------+---------+----------+\n",
      "| 1200001|            Daegu|         Nam-gu| true|  Shincheonji Church|     4511| 35.84008|  128.5667|\n",
      "| 1200009|            Daegu|              -|false|contact with patient|      917|        -|         -|\n",
      "| 1200010|            Daegu|              -|false|                 etc|      747|        -|         -|\n",
      "| 6000001| Gyeongsangbuk-do|from other city| true|  Shincheonji Church|      566|        -|         -|\n",
      "| 2000020|      Gyeonggi-do|              -|false|     overseas inflow|      305|        -|         -|\n",
      "| 1000036|            Seoul|              -|false|     overseas inflow|      298|        -|         -|\n",
      "| 1200002|            Daegu|   Dalseong-gun| true|Second Mi-Ju Hosp...|      196|35.857375|128.466651|\n",
      "| 6000012| Gyeongsangbuk-do|              -|false|contact with patient|      190|        -|         -|\n",
      "| 1000037|            Seoul|              -|false|contact with patient|      162|        -|         -|\n",
      "| 1000001|            Seoul|     Yongsan-gu| true|       Itaewon Clubs|      139|37.538621|126.992652|\n",
      "| 6000013| Gyeongsangbuk-do|              -|false|                 etc|      133|        -|         -|\n",
      "| 1200003|            Daegu|         Seo-gu| true|Hansarang Convale...|      124|35.885592|128.556649|\n",
      "| 1000002|            Seoul|      Gwanak-gu| true|             Richway|      119| 37.48208|126.901384|\n",
      "| 6000002| Gyeongsangbuk-do|   Cheongdo-gun| true|Cheongdo Daenam H...|      119| 35.64887|  128.7368|\n",
      "| 4100001|Chungcheongnam-do|     Cheonan-si| true|gym facility in C...|      103| 36.81503|  127.1139|\n",
      "| 1200004|            Daegu|   Dalseong-gun| true|Daesil Convalesce...|      101|35.857393|128.466653|\n",
      "| 1000038|            Seoul|              -|false|                 etc|      100|        -|         -|\n",
      "| 1000003|            Seoul|        Guro-gu| true| Guro-gu Call Center|       95|37.508163|126.884387|\n",
      "| 2000022|      Gyeonggi-do|              -|false|                 etc|       84|        -|         -|\n",
      "| 1400005|          Incheon|              -|false|     overseas inflow|       68|        -|         -|\n",
      "+--------+-----------------+---------------+-----+--------------------+---------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sorting: use \"asc\" for ascending and \"desc\" for descending order\n",
    "\n",
    "cases.sort(F.desc(\"confirmed\")).show()\n",
    "\n",
    "# alternative\n",
    "#cases.orderBy(F.desc(\"confirmed\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use groupBy function with a spark DataFrame, too. Pretty much same as the pandas groupBy with the exception that you will need to import pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         province|count|\n",
      "+-----------------+-----+\n",
      "|            Seoul|   38|\n",
      "|      Gyeonggi-do|   22|\n",
      "| Gyeongsangbuk-do|   13|\n",
      "| Gyeongsangnam-do|   12|\n",
      "|            Busan|   10|\n",
      "|          Daejeon|   10|\n",
      "|            Daegu|   10|\n",
      "|Chungcheongnam-do|    8|\n",
      "|       Gangwon-do|    8|\n",
      "|Chungcheongbuk-do|    7|\n",
      "|          Incheon|    7|\n",
      "|           Sejong|    6|\n",
      "|     Jeollabuk-do|    5|\n",
      "|     Jeollanam-do|    5|\n",
      "|          Gwangju|    5|\n",
      "|            Ulsan|    4|\n",
      "|          Jeju-do|    4|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grouping operations: cases by province\n",
    "\n",
    "cases.groupBy(\"province\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don’t like the new grouped column names, you can use the alias keyword to rename columns in the agg command itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+-------------+-------------+\n",
      "|        province|           city|tot confirmed|max confirmed|\n",
      "+----------------+---------------+-------------+-------------+\n",
      "|Gyeongsangnam-do|       Jinju-si|            9|            9|\n",
      "|           Seoul|        Guro-gu|          139|           95|\n",
      "|           Seoul|     Gangnam-gu|           18|            7|\n",
      "|         Daejeon|              -|          100|           55|\n",
      "|    Jeollabuk-do|from other city|            6|            3|\n",
      "|Gyeongsangnam-do|Changnyeong-gun|            7|            7|\n",
      "|           Seoul|              -|          561|          298|\n",
      "|         Jeju-do|from other city|            1|            1|\n",
      "|Gyeongsangbuk-do|              -|          345|          190|\n",
      "|Gyeongsangnam-do|   Geochang-gun|           18|           10|\n",
      "+----------------+---------------+-------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grouping operations: tot and max confirmed by prov-city\n",
    "\n",
    "cases.groupBy([\"province\",\"city\"]).agg(\n",
    "    F.sum(\"confirmed\").alias(\"tot confirmed\"),\n",
    "    F.max(\"confirmed\").alias(\"max confirmed\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use .withcolumn along with PySpark SQL functions (like When/Othewise) to create a new column. In essence, you can find String functions, Date functions, and Math functions already implemented using Spark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           city|\n",
      "+---------------+\n",
      "|     Gangnam-gu|\n",
      "|     Cheonan-si|\n",
      "|from other city|\n",
      "|      Anyang-si|\n",
      "|      Gwanak-gu|\n",
      "|     Yongsan-gu|\n",
      "|        Dong-gu|\n",
      "|         Sejong|\n",
      "|     Gangseo-gu|\n",
      "|       Wonju-si|\n",
      "|     Suyeong-gu|\n",
      "|   Geochang-gun|\n",
      "|           null|\n",
      "|  Dongdaemun-gu|\n",
      "|     Dongnae-gu|\n",
      "|         Jin-gu|\n",
      "|     Yangsan-si|\n",
      "|    Changwon-si|\n",
      "|         Nam-gu|\n",
      "|   Gyeongsan-si|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create new column by replacing \"-\" with \"unknown\" in all cases column\n",
    "# if else: when / otherwise\n",
    "\n",
    "for c in cases.columns:\n",
    "    cases = cases.withColumn(\n",
    "        c, F.when(F.col(c)==\"-\", None).otherwise(F.col(c))\n",
    "    )\n",
    "\n",
    "cases.select(\"city\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache / Persist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark works on the lazy execution principle. What that means is that nothing really gets executed until you use an action function like the .count() on a dataframe. And if you do call a .count() function, it generally helps to cache at this step. So you might want to cache() or persist() your dataframes when you do a .count() operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist data in memory or on disk (see persist options to specify persistance at disk level)\n",
    "cases.count()\n",
    "cases = cases.persist()\n",
    "# alternative\n",
    "#cases.persist().count()\n",
    "\n",
    "# alternatively, use cache to persist only in memory\n",
    "#cases.cache().count())\n",
    "#cases.count()\n",
    "#cases = cases.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remeber to unpersist data when not needed anymore\n",
    "#cases.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+-----+----------------+---------+---+----+\n",
      "| case_id|province|city|group|infection_source|confirmed|lat|long|\n",
      "+--------+--------+----+-----+----------------+---------+---+----+\n",
      "|       0|       0|  53|    0|               0|        0|109| 109|\n",
      "+--------+--------+----+-----+----------------+---------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count nulls by column\n",
    "\n",
    "cases.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in cases.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more advanced processing: Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window functions allow to create new columns based on groups of values avoiding for loops on data groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------------+-----+--------------------+---------+---------+----------+-----------------+\n",
      "| case_id|         province|           city|group|    infection_source|confirmed|      lat|      long|tot_cases_by_prov|\n",
      "+--------+-----------------+---------------+-----+--------------------+---------+---------+----------+-----------------+\n",
      "| 1700001|           Sejong|         Sejong| true|Ministry of Ocean...|       31|36.504713|127.265172|               49|\n",
      "| 1700002|           Sejong|         Sejong| true|gym facility in S...|        8| 36.48025|   127.289|               49|\n",
      "| 1700003|           Sejong|from other city| true|  Shincheonji Church|        1|     null|      null|               49|\n",
      "| 1700004|           Sejong|           null|false|     overseas inflow|        5|     null|      null|               49|\n",
      "| 1700005|           Sejong|           null|false|contact with patient|        3|     null|      null|               49|\n",
      "| 1700006|           Sejong|           null|false|                 etc|        1|     null|      null|               49|\n",
      "| 1600001|            Ulsan|from other city| true|  Shincheonji Church|       16|     null|      null|               51|\n",
      "| 1600002|            Ulsan|           null|false|     overseas inflow|       25|     null|      null|               51|\n",
      "| 1600003|            Ulsan|           null|false|contact with patient|        3|     null|      null|               51|\n",
      "| 1600004|            Ulsan|           null|false|                 etc|        7|     null|      null|               51|\n",
      "| 4000001|Chungcheongbuk-do|     Goesan-gun| true|Goesan-gun Jangye...|       11| 36.82422|  127.9552|               60|\n",
      "| 4000002|Chungcheongbuk-do|from other city| true|       Itaewon Clubs|        9|     null|      null|               60|\n",
      "| 4000003|Chungcheongbuk-do|from other city| true| Guro-gu Call Center|        2|     null|      null|               60|\n",
      "| 4000004|Chungcheongbuk-do|from other city| true|  Shincheonji Church|        6|     null|      null|               60|\n",
      "| 4000005|Chungcheongbuk-do|           null|false|     overseas inflow|       13|     null|      null|               60|\n",
      "+--------+-----------------+---------------+-----+--------------------+---------+---------+----------+-----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summing over groups across partitions\n",
    "w_sum = Window.partitionBy(\"province\")\n",
    "\n",
    "cases = cases.withColumn(\"tot_cases_by_prov\", F.sum(\"confirmed\").over(w_sum))\n",
    "cases.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+-----+--------------------+---------+-----------------+------------------+\n",
      "| case_id|province|           city|group|    infection_source|confirmed|tot_cases_by_prov|rank_cases_by_prov|\n",
      "+--------+--------+---------------+-----+--------------------+---------+-----------------+------------------+\n",
      "| 1700001|  Sejong|         Sejong| true|Ministry of Ocean...|       31|               49|                 1|\n",
      "| 1700002|  Sejong|         Sejong| true|gym facility in S...|        8|               49|                 2|\n",
      "| 1700004|  Sejong|           null|false|     overseas inflow|        5|               49|                 3|\n",
      "| 1700005|  Sejong|           null|false|contact with patient|        3|               49|                 4|\n",
      "| 1700003|  Sejong|from other city| true|  Shincheonji Church|        1|               49|                 5|\n",
      "| 1700006|  Sejong|           null|false|                 etc|        1|               49|                 5|\n",
      "| 1600002|   Ulsan|           null|false|     overseas inflow|       25|               51|                 1|\n",
      "| 1600001|   Ulsan|from other city| true|  Shincheonji Church|       16|               51|                 2|\n",
      "| 1600004|   Ulsan|           null|false|                 etc|        7|               51|                 3|\n",
      "| 1600003|   Ulsan|           null|false|contact with patient|        3|               51|                 4|\n",
      "+--------+--------+---------------+-----+--------------------+---------+-----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ranking over groups across partitions\n",
    "w_rank = Window().partitionBy('province').orderBy(F.desc('confirmed'))\n",
    "\n",
    "cases_ranked = cases.withColumn(\n",
    "    \"rank_cases_by_prov\", F.rank().over(w_rank)\n",
    ").drop(*[\"lat\", \"long\"])\n",
    "\n",
    "cases_ranked.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|           city|confirmed|\n",
      "+---------------+---------+\n",
      "|         Sejong|       31|\n",
      "|from other city|       17|\n",
      "|from other city|      566|\n",
      "|         Nam-gu|     4511|\n",
      "|from other city|       32|\n",
      "|     Dongnae-gu|       39|\n",
      "|     Cheonan-si|      103|\n",
      "+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cases_ranked.filter(F.col(\"rank_cases_by_prov\")==1).select([\"city\", \"confirmed\"]).dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset time range: 2020-01-20 - 2020-06-30\n"
     ]
    }
   ],
   "source": [
    "start = time_province.select(\"date\").rdd.min()[0]\n",
    "end = time_province.select(\"date\").rdd.max()[0]\n",
    "\n",
    "print(\"Dataset time range: {} - {}\".format(start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+---------+--------+--------+-----+\n",
      "|      date|time|province|confirmed|released|deceased|lag_7|\n",
      "+----------+----+--------+---------+--------+--------+-----+\n",
      "|2020-03-11|   0|  Sejong|       10|       0|       0|    1|\n",
      "|2020-03-12|   0|  Sejong|       15|       0|       0|    1|\n",
      "|2020-03-13|   0|  Sejong|       32|       0|       0|    1|\n",
      "|2020-03-14|   0|  Sejong|       38|       0|       0|    2|\n",
      "|2020-03-15|   0|  Sejong|       39|       0|       0|    3|\n",
      "|2020-03-16|   0|  Sejong|       40|       0|       0|    6|\n",
      "|2020-03-17|   0|  Sejong|       40|       0|       0|    8|\n",
      "|2020-03-18|   0|  Sejong|       41|       0|       0|   10|\n",
      "|2020-03-19|   0|  Sejong|       41|       0|       0|   15|\n",
      "|2020-03-20|   0|  Sejong|       41|       0|       0|   32|\n",
      "|2020-03-21|   0|  Sejong|       41|       2|       0|   38|\n",
      "|2020-03-22|   0|  Sejong|       41|       3|       0|   39|\n",
      "|2020-03-23|   0|  Sejong|       42|       3|       0|   40|\n",
      "|2020-03-24|   0|  Sejong|       42|       3|       0|   40|\n",
      "|2020-03-25|   0|  Sejong|       44|       3|       0|   41|\n",
      "|2020-03-26|   0|  Sejong|       44|       8|       0|   41|\n",
      "|2020-03-27|   0|  Sejong|       44|       9|       0|   41|\n",
      "|2020-03-28|   0|  Sejong|       44|       9|       0|   41|\n",
      "|2020-03-29|   0|  Sejong|       46|      11|       0|   41|\n",
      "|2020-03-30|   0|  Sejong|       46|      12|       0|   42|\n",
      "+----------+----+--------+---------+--------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lagging over groups across partitions\n",
    "w_lag = Window().partitionBy(['province']).orderBy('date')\n",
    "\n",
    "time_province = time_province.withColumn(\"lag_7\",F.lag(\"confirmed\", 7).over(w_lag))\n",
    "time_province.filter(time_province.date>'2020-03-10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+---------+--------+--------+-----+----------------+\n",
      "|      date|time|province|confirmed|released|deceased|lag_7|roll_7_confirmed|\n",
      "+----------+----+--------+---------+--------+--------+-----+----------------+\n",
      "|2020-03-11|   0|  Sejong|       10|       0|       0|    1|            4.43|\n",
      "|2020-03-12|   0|  Sejong|       15|       0|       0|    1|            6.43|\n",
      "|2020-03-13|   0|  Sejong|       32|       0|       0|    1|           10.86|\n",
      "|2020-03-14|   0|  Sejong|       38|       0|       0|    2|            16.0|\n",
      "|2020-03-15|   0|  Sejong|       39|       0|       0|    3|           21.14|\n",
      "|2020-03-16|   0|  Sejong|       40|       0|       0|    6|            26.0|\n",
      "|2020-03-17|   0|  Sejong|       40|       0|       0|    8|           30.57|\n",
      "|2020-03-18|   0|  Sejong|       41|       0|       0|   10|            35.0|\n",
      "|2020-03-19|   0|  Sejong|       41|       0|       0|   15|           38.71|\n",
      "|2020-03-20|   0|  Sejong|       41|       0|       0|   32|            40.0|\n",
      "+----------+----+--------+---------+--------+--------+-----+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rolling over group using an aggregation (mean) over ther last 7 days\n",
    "# include current day: rowsBetween(-6,0)\n",
    "# exclude current day: rowsBetween(-7,-1)\n",
    "\n",
    "w_roll = Window().partitionBy(['province']).orderBy('date').rowsBetween(-6,0)\n",
    "\n",
    "time_province = time_province.withColumn(\n",
    "    \"roll_7_confirmed\", F.round(F.mean(\"confirmed\").over(w_roll),2)\n",
    ")\n",
    "time_province.filter(time_province.date>'2020-03-10').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: integer (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- confirmed: integer (nullable = true)\n",
      " |-- released: integer (nullable = true)\n",
      " |-- deceased: integer (nullable = true)\n",
      " |-- lag_7: integer (nullable = true)\n",
      " |-- roll_7_confirmed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_province.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more advanced processing: UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want to do complicated things with a column or multiple columns. While Spark SQL functions do solve many use cases, when it comes to column creation, we can create Spark UDFs to build more matured or custom Python functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases.count()\n",
    "cases = cases.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|confirmed_level|count|\n",
      "+---------------+-----+\n",
      "|            low|  143|\n",
      "|           high|   31|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_confirmed_level(confirmed):\n",
    "    \"\"\"\n",
    "    Assigns \"high\" category if confirmed cases are \n",
    "    above 50 otherwise \"low\"\n",
    "    \"\"\"\n",
    "    if confirmed < 50: \n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'high'\n",
    "    \n",
    "#convert to a UDF Function by passing in the function and return type of function\n",
    "confirmed_udf = F.udf(lambda x: get_confirmed_level(x), StringType())\n",
    "\n",
    "cases = cases.withColumn(\"confirmed_level\", confirmed_udf(F.col(\"confirmed\")))\n",
    "cases.groupBy(\"confirmed_level\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+\n",
      "|year_month|released|deceased|\n",
      "+----------+--------+--------+\n",
      "|    202001|       0|       0|\n",
      "|    202002|     311|      85|\n",
      "|    202003|   57024|    2587|\n",
      "|    202004|  226480|    6525|\n",
      "|    202005|  294969|    8081|\n",
      "|    202006|  309949|    8326|\n",
      "+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def year_month(date):\n",
    "    \"\"\"\n",
    "    Extract year_month from datetime.date object \n",
    "    as yyyymm format and nteger type\n",
    "    \"\"\"\n",
    "    if date is not None:\n",
    "        month = str(date.month)\n",
    "        year = str(date.year)\n",
    "        if len(month) < 2:\n",
    "            year_month_var = year + \"0\" + month\n",
    "        else:\n",
    "            year_month_var = year + month\n",
    "        return int(year_month_var)\n",
    "\n",
    "        \n",
    "year_month_udf = F.udf(year_month, IntegerType())\n",
    "\n",
    "time_province = time_province.withColumn(\"date\", F.to_date(F.col(\"date\")))\n",
    "time_province = time_province.withColumn(\"year_month\", year_month_udf(F.col(\"date\")))\n",
    "\n",
    "time_province.groupBy(\"year_month\").agg(\n",
    "    F.sum(\"released\").alias(\"released\"),\n",
    "    F.sum(\"deceased\").alias(\"deceased\")\n",
    ").orderBy(\"year_month\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort Merge Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Sort Merge Join enables an all-to-all communication strategy among the nodes: the Driver Node will orchestrate the Executors, each of which will hold a particular set of joining keys. Before running the actual operation, the partitions are first sorted (this operation is obviously heavy itself). As you can imagine this kind of strategy can be expensive: nodes need to use the network to share data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join records 174\n",
      "root\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |--  case_id: integer (nullable = true)\n",
      " |-- group: boolean (nullable = true)\n",
      " |-- infection_source: string (nullable = true)\n",
      " |-- confirmed: integer (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- long: string (nullable = true)\n",
      " |-- tot_cases_by_prov: long (nullable = true)\n",
      " |-- confirmed_level: string (nullable = true)\n",
      " |-- code: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elementary_school_count: integer (nullable = true)\n",
      " |-- kindergarten_count: integer (nullable = true)\n",
      " |-- university_count: integer (nullable = true)\n",
      " |-- academy_ratio: double (nullable = true)\n",
      " |-- elderly_population_ratio: double (nullable = true)\n",
      " |-- elderly_alone_ratio: double (nullable = true)\n",
      " |-- nursing_home_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cases_with_region = cases.join(regions, ['province','city'], how='left')\n",
    "print(\"Join records {}\".format(cases_with_region.count()))\n",
    "cases_with_region.persist()\n",
    "\n",
    "cases_with_region.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast / Map Side Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a boradcast join when you face a scenario where you need to join a very big table (about 1B Rows) with a very small table (about 100–200 rows). In such type of join, you broadcast the small table to each machine/node when you perform a join with the big table. Broadcasting operation is itself quite expensive (it means that all the nodes need to receive a copy of the table), so it’s not surprising that if we increase the amount of executors that need to receive the table, we increase the broadcasting cost. If we have more executors available, a sort merge join may be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join records 174\n",
      "root\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |--  case_id: integer (nullable = true)\n",
      " |-- group: boolean (nullable = true)\n",
      " |-- infection_source: string (nullable = true)\n",
      " |-- confirmed: integer (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- long: string (nullable = true)\n",
      " |-- tot_cases_by_prov: long (nullable = true)\n",
      " |-- confirmed_level: string (nullable = true)\n",
      " |-- code: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elementary_school_count: integer (nullable = true)\n",
      " |-- kindergarten_count: integer (nullable = true)\n",
      " |-- university_count: integer (nullable = true)\n",
      " |-- academy_ratio: double (nullable = true)\n",
      " |-- elderly_population_ratio: double (nullable = true)\n",
      " |-- elderly_alone_ratio: double (nullable = true)\n",
      " |-- nursing_home_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cases_reg_broad = cases.join(broadcast(regions), ['province','city'], how='left')\n",
    "\n",
    "print(\"Join records {}\".format(cases_with_region.count()))\n",
    "cases_reg_broad.persist()\n",
    "\n",
    "cases_reg_broad.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converto to Pandas dataframe and save as csv file\n",
    "cases_with_region.toPandas().to_csv(\"saved_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, save as parquet file\n",
    "\n",
    "# first remove blank space from column name to avoid column writing errors\n",
    "#cases_with_region = cases_with_region.withColumnRenamed(\" case_id\", \"case_id\")\n",
    "cases = cases.withColumnRenamed(\" case_id\", \"case_id\").persist()\n",
    "cases.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(\"cases_with_region.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpersist data after saving them\n",
    "cases_with_region.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coalesce / Repartition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With too few partitions you will not utilize all of the cores available in the cluster.\n",
    "\n",
    "With too many partitions there will be excessive overhead in managing many small tasks.\n",
    "\n",
    "Between the two the first one is far more impactful on performance. Scheduling too many smalls tasks is a relatively small impact at this point for partition counts below 1000. If you have on the order of tens of thousands of partitions then spark gets very slow.\n",
    "\n",
    "Have your number of partitions set to 3 or 4 times the number of CPU cores in your cluster so that the work gets distributed more evenly among the available CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of partitions in a data frame\n",
    "cases_with_region.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the distribution of records in a partition by using the glom function\n",
    "#cases_with_region.rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coalesce partitions: can only reduce the number of partitions\n",
    "\n",
    "cases_with_region = cases_with_region.coalesce(4)\n",
    "cases_with_region.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartition by cores: increase or decrease the number of partitions\n",
    "cores = 8\n",
    "n = 3\n",
    "partitions = cores*n\n",
    "\n",
    "cases_with_region = cases_with_region.repartition(\"partitions\")\n",
    "cases_with_region.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartition by column: sets as many partitions as column levels\n",
    "\n",
    "cases_with_region = cases_with_region.repartition(\"province\")\n",
    "cases_with_region.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
