{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: introduction to data analysis with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_HOME = \"C:\\Spark\\spark-3.0.1-bin-hadoop2.7\"\n",
    "findspark.init(SPARK_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration & Intialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SparkContext — provides connection to Spark with the ability to create RDDs\n",
    "* SQLContext — provides connection to Spark with the ability to run SQL queries on data\n",
    "* SparkSession — all-encompassing context which includes coverage for SparkContext, SQLContext and HiveContext. SparkSession is the entry point for programming Spark applications. It let you interact with DataSet and DataFrame APIs provided by Spark. We set the application name by calling appName. The getOrCreate() method either returns a new SparkSession of the app or returns the existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "spark = SparkSession.builder.appName(\"data analysis with PySaprk\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SparkContext instance which allows the Spark Application to access \n",
    "# Spark Cluster with the help of a resource manager which is usually YARN or Mesos\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SQLContext instance to access the SQL query engine built on top of Spark\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by loading the files in our dataset using the spark.read.load command. This command reads parquet files, which is the default file format for spark, but you can add the parameter format to read other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covid cases dataset\n",
    "cases = spark.read.load(\n",
    "    \"data/Case.csv\", \n",
    "    format=\"csv\", \n",
    "    sep=\",\",\n",
    "    inferSchema=\"true\", \n",
    "    header=\"true\"\n",
    ")\n",
    "cases.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases timing dataset\n",
    "time_province = spark.read.load(\n",
    "    \"data/TimeProvince.csv\", \n",
    "    format=\"csv\", \n",
    "    sep=\",\",\n",
    "    inferSchema=\"true\", \n",
    "    header=\"true\"\n",
    ")\n",
    "time_province.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regions = spark.read.load(\n",
    "    \"data/Region.csv\",\n",
    "    format=\"csv\", \n",
    "    sep=\",\", \n",
    "    inferSchema=\"true\", \n",
    "    header=\"true\"\n",
    ")\n",
    "regions.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative reading options\n",
    "#df = spark.read.csv(\"Case.csv\", header=True, inferSchema=True)\n",
    "#df = spark.read.format('csv').options(header=True,inferSchema=True).load(\"Case.csv\")\n",
    "\n",
    "# for other file formats:\n",
    "#df = spark.read.text(path_to_file)\n",
    "#df = spark.read.json(path_to_file)\n",
    "#df = spark.read.parquet(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# know data schema\n",
    "cases.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a descriptive overview of data fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe data\n",
    "describe_cases = cases.select(\"province\", \"city\", \"infection_case\", \"confirmed\", \"latitude\", \"longitude\")\n",
    "describe_cases.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter a data frame using multiple conditions using AND(&), OR(|) and NOT(~) conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter columns\n",
    "seoul_cases = cases.filter(F.col(\"province\")==\"Seoul\")\n",
    "seoul_cases.count()\n",
    "\n",
    "#alternative\n",
    "#seoul_cases = cases.where(F.col(\"province\")==\"Seoul\")\n",
    "#seoul_cases.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases.where((cases.confirmed>100) & (cases.province=='Daegu')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select a subset of columns using the select keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns \n",
    "cases.select([\"province\", \"city\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change a single column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns: exisitng left, new right\n",
    "cases = cases\\\n",
    "    .withColumnRenamed(\"latitude\", \"lat\")\\\n",
    "    .withColumnRenamed(\"longitude\", \"long\")\\\n",
    "    .withColumnRenamed(\"infection_case\", \"infection_source\")\n",
    "\n",
    "cases.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort data by increasing (deafult) or decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting: use \"asc\" for ascending and \"desc\" for descending order\n",
    "\n",
    "cases.sort(F.desc(\"confirmed\")).show()\n",
    "\n",
    "# alternative\n",
    "#cases.orderBy(F.desc(\"confirmed\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use groupBy function with a spark DataFrame, too. Pretty much same as the pandas groupBy with the exception that you will need to import pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping operations: cases by province\n",
    "\n",
    "cases.groupBy(\"province\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don’t like the new grouped column names, you can use the alias keyword to rename columns in the agg command itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping operations: tot and max confirmed by prov-city\n",
    "\n",
    "cases.groupBy([\"province\",\"city\"]).agg(\n",
    "    F.sum(\"confirmed\").alias(\"tot confirmed\"),\n",
    "    F.max(\"confirmed\").alias(\"max confirmed\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use .withcolumn along with PySpark SQL functions (like When/Othewise) to create a new column. In essence, you can find String functions, Date functions, and Math functions already implemented using Spark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column by replacing \"-\" with \"unknown\" in all cases column\n",
    "# if else: when / otherwise\n",
    "\n",
    "for c in cases.columns:\n",
    "    cases = cases.withColumn(\n",
    "        c, F.when(F.col(c)==\"-\", None).otherwise(F.col(c))\n",
    "    )\n",
    "\n",
    "cases.select(\"city\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache / Persist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark works on the lazy execution principle. What that means is that nothing really gets executed until you use an action function like the .count() on a dataframe. And if you do a .count function, it generally helps to cache at this step. So you might want to cache() or persist() your dataframes when you do a .count() operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist data in memory\n",
    "cases.count()\n",
    "cases = cases.persist()\n",
    "# alternative\n",
    "#cases.persist().count()\n",
    "\n",
    "# alternative\n",
    "#cases.cache().count())\n",
    "#cases.count()\n",
    "#cases = cases.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to unpersist:\n",
    "#cases.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nulls by column\n",
    "\n",
    "cases.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in cases.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more advanced processing: Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window functions allow to create new columns based on groups of values avoiding for loops on data groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summing\n",
    "w_sum = Window.partitionBy(\"province\")\n",
    "\n",
    "cases = cases.withColumn(\"tot_cases_by_prov\", F.sum(\"confirmed\").over(w_sum))\n",
    "cases.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking\n",
    "w_rank = Window().partitionBy('province').orderBy(F.desc('confirmed'))\n",
    "\n",
    "cases_ranked = cases.withColumn(\n",
    "    \"rank_cases_by_prov\", F.rank().over(w_rank)\n",
    ").drop(*[\"lat\", \"long\"])\n",
    "\n",
    "cases_ranked.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_ranked.filter(F.col(\"rank_cases_by_prov\")==1).select([\"city\", \"confirmed\"]).dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time_province.select(\"date\").rdd.min()[0]\n",
    "end = time_province.select(\"date\").rdd.max()[0]\n",
    "\n",
    "print(\"Dataset time range: {} - {}\".format(start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lagging\n",
    "w_lag = Window().partitionBy(['province']).orderBy('date')\n",
    "\n",
    "time_province = time_province.withColumn(\"lag_7\",F.lag(\"confirmed\", 7).over(w_lag))\n",
    "time_province.filter(time_province.date>'2020-03-10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling aggregation (mean) ove ther last 7 days\n",
    "# include current day: rowsBetween(-6,0)\n",
    "# exclude current day: rowsBetween(-7,-1)\n",
    "\n",
    "w_roll = Window().partitionBy(['province']).orderBy('date').rowsBetween(-6,0)\n",
    "\n",
    "time_province = time_province.withColumn(\n",
    "    \"roll_7_confirmed\", F.round(F.mean(\"confirmed\").over(w_roll),2)\n",
    ")\n",
    "time_province.filter(time_province.date>'2020-03-10').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_province.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more advanced processing: UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want to do complicated things with a column or multiple columns. While Spark SQL functions do solve many use cases, when it comes to column creation, we can create Spark UDFs to build more matured Python functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases.count()\n",
    "cases = cases.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confirmed_level(confirmed):\n",
    "    \"\"\"\n",
    "    Assigns \"high\" category if confirmed cases are \n",
    "    above 50 otherwise \"low\"\n",
    "    \"\"\"\n",
    "    if confirmed < 50: \n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'high'\n",
    "    \n",
    "#convert to a UDF Function by passing in the function and return type of function\n",
    "confirmed_udf = F.udf(get_confirmed_level, StringType())\n",
    "\n",
    "cases = cases.withColumn(\"confirmed_level\", confirmed_udf(F.col(\"confirmed\")))\n",
    "cases.groupBy(\"confirmed_level\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_month(date):\n",
    "    \"\"\"\n",
    "    Extract year_month from datetime.date object \n",
    "    as yyyymm format and nteger type\n",
    "    \"\"\"\n",
    "    if date is not None:\n",
    "        month = str(date.month)\n",
    "        year = str(date.year)\n",
    "        if len(month) < 2:\n",
    "            year_month_var = year + \"0\" + month\n",
    "        else:\n",
    "            year_month_var = year + month\n",
    "        return int(year_month_var)\n",
    "\n",
    "        \n",
    "year_month_udf = F.udf(year_month, IntegerType())\n",
    "\n",
    "time_province = time_province.withColumn(\"date\", F.to_date(F.col(\"date\")))\n",
    "time_province = time_province.withColumn(\"year_month\", year_month_udf(F.col(\"date\")))\n",
    "\n",
    "time_province.groupBy(\"year_month\").agg(\n",
    "    F.sum(\"released\").alias(\"released\"),\n",
    "    F.sum(\"deceased\").alias(\"deceased\")\n",
    ").orderBy(\"year_month\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort Merge Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Sort Merge Join enables an all-to-all communication strategy among the nodes: the Driver Node will orchestrate the Executors, each of which will hold a particular set of joining keys. Before running the actual operation, the partitions are first sorted (this operation is obviously heavy itself). As you can imagine this kind of strategy can be expensive: nodes need to use the network to share data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_with_region = cases.join(regions, ['province','city'], how='left')\n",
    "print(\"Join records {}\".format(cases_with_region.count()))\n",
    "cases_with_region.persist()\n",
    "\n",
    "cases_with_region.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast/Map Side Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a boradcast join when you face a scenario where you need to join a very big table (about 1B Rows) with a very small table (about 100–200 rows). In such type of join, you broadcast the small table to each machine/node when you perform a join with the big table. Broadcasting operation is itself quite expensive (it means that all the nodes need to receive a copy of the table), so it’s not surprising that if we increase the amount of executors that need to receive the table, we increase the broadcasting cost. If we have more executors available, a sort merge join may be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_reg_broad = cases.join(broadcast(regions), ['province','city'], how='left')\n",
    "print(\"Join records {}\".format(cases_with_region.count()))\n",
    "cases_reg_broad.persist()\n",
    "\n",
    "cases_reg_broad.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove blank space from column name to avoid writing errors\n",
    "#cases_with_region = cases_with_region.withColumnRenamed(\" case_id\", \"case_id\")\n",
    "\n",
    "#converto to Pandas dataframe and save as csv file\n",
    "cases_with_region.toPandas().to_csv(\"saved_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['HADOOP_HOME'] = \"C:/hadoop\"\n",
    "sys.path.append(\"C:/hadoop/bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, save as parquet\n",
    "cases = cases.withColumnRenamed(\" case_id\", \"case_id\").persist()\n",
    "cases.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(\"cases_with_region.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpersist data after saving them\n",
    "cases_with_region.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coalesce / Repartition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With too few partitions You will not utilize all of the cores available in the cluster.\n",
    "\n",
    "With too many partitions There will be excessive overhead in managing many small tasks.\n",
    "\n",
    "Between the two the first one is far more impactful on performance. Scheduling too many smalls tasks is a relatively small impact at this point for partition counts below 1000. If you have on the order of tens of thousands of partitions then spark gets very slow.\n",
    "\n",
    "Have your number of partitions set to 3 or 4 times the number of CPU cores in your cluster so that the work gets distributed more evenly among the available CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of partitions in a data frame\n",
    "cases_with_region.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the distribution of records in a partition by using the glom function\n",
    "#cases_with_region.rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coalesce partitions\n",
    "\n",
    "cases_with_region = cases_with_region.coalesce(4)\n",
    "cases_with_region.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition by cores\n",
    "cores = 8\n",
    "n = 3\n",
    "partitions = cores*n\n",
    "\n",
    "cases_with_region = cases_with_region.repartition(partitions)\n",
    "cases_with_region.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition by column\n",
    "\n",
    "cases_with_region = cases_with_region.repartition(\"province\")\n",
    "cases_with_region.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
